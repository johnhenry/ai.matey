# ai.matey Roadmap

Development roadmap and future plans for the Universal AI Adapter System.

## Current State (Monorepo Consolidated)

**Monorepo Structure:**
- 23 packages (consolidated from 72)
- 165 TypeScript source files
- 32,019 lines of TypeScript code
- 43 test files with comprehensive coverage
- Turbo-based build system with caching
- Dual-format distribution: ESM and CommonJS
- Full TypeScript declarations

**Core Foundation:**
- ‚úÖ Universal IR (Intermediate Representation) - comprehensive format spec
- ‚úÖ Bridge architecture with middleware support
- ‚úÖ Router with 7 routing strategies (explicit, model-based, cost-optimized, latency-optimized, round-robin, random, custom)
- ‚úÖ Circuit breaker pattern with health checking
- ‚úÖ Fallback chains (sequential and parallel)
- ‚úÖ Provider-agnostic request/response handling

**Adapters:**
- ‚úÖ 7 Frontend Adapters (OpenAI, Anthropic, Gemini, Mistral, Ollama, Chrome AI, Generic)
- ‚úÖ 24 Backend Providers (OpenAI, Anthropic, Gemini, Mistral, Cohere, Groq, Ollama, AI21, Anyscale, AWS Bedrock, Azure OpenAI, Cerebras, Cloudflare, DeepInfra, DeepSeek, Fireworks, HuggingFace, LMStudio, NVIDIA, OpenRouter, Perplexity, Replicate, Together AI, X AI)
- ‚úÖ 3 Browser Backends (Chrome AI, Function-based, Mock provider)
- ‚úÖ Native adapters (Apple Silicon, node-llamacpp)

**Middleware & Cross-Cutting Concerns:**
- ‚úÖ 10 Middleware types:
  - logging - Request/response logging
  - telemetry - Metrics collection
  - opentelemetry - Distributed tracing (OpenTelemetry standard)
  - caching - Response caching
  - retry - Automatic retries with backoff
  - transform - Request/response transforms
  - security - Rate limiting & security
  - cost-tracking - Usage & cost tracking
  - validation - Input validation & sanitization
  - conversation-history - Context management
- ‚úÖ Extensible middleware pipeline
- ‚úÖ Custom middleware support

**HTTP Integration:**
- ‚úÖ 6 Framework adapters (Express, Fastify, Hono, Koa, Node.js, Deno)
- ‚úÖ Shared HTTP utilities (auth, CORS, error-handler, health-check, rate-limiter, streaming-handler)
- ‚úÖ OpenAI-compatible API endpoints

**React Integration:**
- ‚úÖ useChat hook (ai.matey.react.core)
- ‚úÖ useCompletion hook (ai.matey.react.core)
- ‚úÖ useObject hook (ai.matey.react.core)
- ‚úÖ useAssistant hook (ai.matey.react.hooks)
- ‚úÖ useStream hook (ai.matey.react.hooks)
- ‚úÖ useTokenCount hook (ai.matey.react.hooks)
- ‚úÖ StreamProvider component (ai.matey.react.stream)
- ‚úÖ Next.js App Router integration (ai.matey.react.nextjs)
- ‚úÖ Server Actions support
- ‚úÖ Client and server utilities

**SDK Wrappers:**
- ‚úÖ OpenAI SDK-compatible wrapper
- ‚úÖ Anthropic SDK-compatible wrapper
- ‚úÖ Chrome AI legacy support
- ‚úÖ AnyMethod wrapper

**CLI Tools:**
- ‚úÖ Proxy server
- ‚úÖ Format converters (request/response)
- ‚úÖ Ollama command emulation (list, ps, pull, run, show)
- ‚úÖ Backend loader
- ‚úÖ Pipeline inspector
- ‚úÖ Model translation utilities

**Testing & Quality:**
- ‚úÖ Request/response fixtures for all providers
- ‚úÖ Test helpers and contract testing utilities
- ‚úÖ Property-based testing setup
- ‚úÖ Coverage thresholds configured
- ‚úÖ Integration tests for cross-provider scenarios

**Developer Experience:**
- ‚úÖ Comprehensive TypeScript types with discriminated unions
- ‚úÖ Type inference from adapters
- ‚úÖ Debug mode with pipeline visibility
- ‚úÖ Performance profiling tools
- ‚úÖ Semantic drift warnings
- ‚úÖ Provenance tracking throughout request chain

## Competitive Positioning & Strategic Priorities

### Market Position

ai.matey occupies a unique position in the AI tooling ecosystem as a **provider-agnostic abstraction layer** with production-grade features. Our competitive advantages include:

**Technical Differentiation:**
- ‚úÖ **Zero runtime dependencies** (unique among comprehensive solutions)
- ‚úÖ **Advanced routing** (7 strategies including cost/latency optimization)
- ‚úÖ **Circuit breaker pattern** (rare in this space)
- ‚úÖ **Universal IR** (provider-agnostic intermediate representation)
- ‚úÖ **HTTP framework integration** (6 frameworks)
- ‚úÖ **React integration** (4 packages with hooks, streaming, Next.js support)
- ‚úÖ **Monorepo architecture** (23 well-organized packages)

**Strategic Focus:**
- ‚úÖ **Provider abstraction** (not orchestration like LangChain)
- ‚úÖ **Backend infrastructure** (not just UI like Vercel AI SDK)
- ‚úÖ **Production reliability** (not just prototyping like LiteLLM.js)
- ‚úÖ **Self-hosted** (not managed service like Portkey)
- ‚úÖ **Library approach** (embedded in your app, not gateway service)

### Key Competitors Analysis

For a comprehensive competitive analysis covering 20+ competitors across 6 market categories, see **[competitive-analysis.md](./competitive-analysis.md)**.

**Quick Summary:**

**1. Vercel AI SDK** - UI Framework
- ‚úÖ **ADDRESSED**: Full React integration matches their hooks
- üéØ **Gap**: Structured output with Zod - HIGH PRIORITY
- ‚≠ê **Our Edge**: Provider abstraction, routing, middleware

**2. LangChain.js** - Orchestration Framework
- **WON'T COMPETE**: Different problem domain (RAG/agents vs provider abstraction)
- **COMPLEMENTARY**: LangChain for orchestration + ai.matey for provider layer

**3. Portkey** - Gateway Service
- **DIFFERENT ARCHITECTURE**: Library (embedded) vs Gateway (proxy)
- ‚≠ê **Our Edge**: Privacy-first, full control, zero external dependencies

**4. Instructor-JS** - Structured Output
- üéØ **Gap**: Zod integration - HIGH PRIORITY
- **COMPLEMENTARY**: Could work together

**5. LiteLLM.js / llm.js** - Simple Wrappers
- ‚≠ê **Our Edge**: Production-grade features, advanced routing, comprehensive middleware
- **Trade-off**: More powerful but higher complexity

See [competitive-analysis.md](./competitive-analysis.md) for detailed positioning, feature comparisons, and strategic recommendations.

### Competitive Gaps & Priorities

**‚úÖ RECENTLY ADDRESSED:**
1. ‚úÖ **React hooks** (`useChat`, `useCompletion`) - Matches Vercel AI SDK
2. ‚úÖ **Next.js integration** - App Router, Server Actions support
3. ‚úÖ **Streaming components** - StreamProvider, hooks

**HIGH Priority (Next Phase):**
1. **Structured output with Zod** (like Instructor-JS `generateObject`)
   - Zod schema ‚Üí tool definitions
   - Runtime validation
   - Type inference
   - Streaming with partial objects
2. **Better documentation** - Interactive examples, more tutorials
3. **Semantic caching** - Unique differentiator (like Portkey)
4. **Embeddings support** - Missing compared to LiteLLM.js

**MEDIUM Priority (Differentiation):**
1. **Guardrails system** - Inspired by Portkey (50+ checks), but via middleware
2. **Agent runtime** - Light orchestration without LangChain complexity
3. **RAG pipeline** - Basic support without full LlamaIndex scope
4. **Geographic routing** - Enterprise feature for global deployments

**LOW Priority (Nice to Have):**
1. **Browser integration** - WebLLM support
2. **VSCode extension** - Developer experience enhancement
3. **Community marketplace** - Long-term ecosystem play

### Strategic Roadmap Alignment

Our roadmap focuses on features that:
1. ‚úÖ **Strengthen core competency** (provider abstraction) - Ongoing
2. ‚úÖ **Fill competitive gaps** (React hooks ‚úÖ DONE, structured output ‚Üí Next phase)
3. ‚úÖ **Add unique value** (semantic caching, middleware pipeline)
4. ‚ùå **Avoid scope creep** (won't compete on full orchestration/RAG)

### Market Positioning Summary

| Feature Area | Vercel AI | LangChain | Portkey | Instructor | ai.matey |
|--------------|-----------|-----------|---------|------------|----------|
| React Integration | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê | ‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Provider Abstraction | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Advanced Routing | ‚≠ê | ‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Structured Output | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê (planned) |
| RAG/Orchestration | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê | ‚≠ê (won't compete) |
| Guardrails | ‚≠ê | ‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê | ‚≠ê‚≠ê (middleware) |
| Self-Hosted | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Zero Dependencies | ‚≠ê‚≠ê | ‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

---

## Upcoming Priorities

### Next Phase: Structured Output & Documentation (Priority: HIGH)

**Structured Output with Zod (closes gap with Instructor-JS & Vercel AI)**
- Zod schema integration
- Schema ‚Üí tool definitions converter
- Runtime validation
- Type inference from schemas
- Streaming with partial objects (progressive hydration)
- `generateObject()` method (like Vercel AI SDK)
- JSON schema validation fallback

**Enhanced Documentation**
- Interactive code examples
- Video tutorials (leveraging existing 42 video synopses in docs/)
- Step-by-step guides for common patterns
- More real-world examples
- API reference improvements
- Migration guides from other frameworks

**Embeddings Support (closes gap with LiteLLM.js)**
- Embedding generation across providers
- Batch embedding support
- Vector dimension normalization
- Embedding cost tracking

### Future Phase: Semantic Caching & Guardrails (Priority: MEDIUM-HIGH)

**Semantic Caching (unique differentiator, inspired by Portkey)**
- Cache by semantic meaning, not exact match
- Cosine similarity matching
- Configurable threshold
- Cache across different provider formats
- Cache invalidation strategies
- Performance: 20x faster than API calls

**Guardrails System (inspired by Portkey's 50+ checks)**
- Middleware-based guardrail architecture
- Pre-built deterministic checks:
  - PII detection (SSN, credit cards, emails, phone numbers)
  - Profanity filtering
  - Code detection (SQL, Python, JavaScript)
  - URL detection
  - Prompt injection detection
- LLM-based checks:
  - Toxicity detection
  - Bias detection
  - Factual consistency
- Configurable actions: deny, log, fallback, retry
- Custom guardrail support

**OpenTelemetry Enhancement** (extends existing implementation)
- Additional integration examples (Jaeger, Zipkin, Datadog, Honeycomb)
- Performance optimization for trace spans
- Enhanced documentation and tutorials

### Later Phase: Agent Runtime & RAG Basics (Priority: MEDIUM)

**Lightweight Agent Runtime (not competing with LangChain)**
- ReAct pattern support
- Basic tool orchestration
- Multi-step reasoning (simple)
- State management (minimal)
- Memory helpers
- **Note:** Focus on provider-agnostic agent flows, not complex multi-actor systems

**Basic RAG Pipeline (not competing with LlamaIndex)**
- Simple document Q&A (not complex retrieval workflows)
- Document chunking helpers
- Basic vector store integration (Pinecone, Weaviate, Qdrant, Chroma)
- Semantic search utilities
- **Note:** Target simple use cases, integrate with existing RAG libraries for advanced features

---

## Long-Term Considerations

**Enhanced Multi-Modal:**
- Audio processing (speech-to-text, text-to-speech)
- Image generation (DALL-E, Stable Diffusion)
- Video understanding
- Advanced vision capabilities

**Enterprise Features:**
- Geographic routing (route to nearest provider for latency)
- Multi-tenancy support
- Advanced rate limiting per tenant
- Audit logging and compliance
- SSO integration
- Service mesh integration

**Performance Optimizations:**
- Request deduplication
- Batch request optimization
- Response compression
- Parallel dispatch improvements

**Developer Experience:**
- VSCode extension with snippets
- Browser debugging extension
- Interactive code playground (web-based)
- "Awesome ai.matey" community list
- Community adapter marketplace

**Advanced Features:**
- Machine learning optimization (learn optimal models from usage)
- Model recommendations (suggest cheaper/better alternatives)
- Dynamic pricing (real-time pricing API integration)
- Advanced capability matching (semantic, not just boolean)
- Prompt template system with versioning

**Ecosystem Expansion:**
- SvelteKit integration
- Vue.js composables
- WebLLM browser integration (hybrid local+cloud)
- Plugin system
- Community adapter registry

---

## Historical Context

This roadmap replaces the previous competitive analysis documents. The old docs/competition directory contained detailed comparisons with:
- Vercel AI SDK
- LangChain.js
- Portkey AI Gateway
- Instructor-JS
- LiteLLM.js
- And 18 other competitors

**Key takeaways from competitive analysis:**
1. React integration closes a major gap with Vercel AI SDK
2. Structured output with Zod is the highest priority remaining gap
3. We should NOT compete on full RAG/orchestration (LangChain's domain)
4. Library approach (vs gateway service like Portkey) is a strategic differentiation
5. Zero dependencies and comprehensive routing are unique strengths

For archived competitive analysis, see git history of docs/competition/.

---

## OpenTelemetry Integration (Detailed)

**Status:** ‚úÖ **IMPLEMENTED** - Available in `ai.matey.middleware.opentelemetry`
**Competitive Priority:** HIGH - Industry standard for observability

### Current Implementation

OpenTelemetry middleware is **already implemented** and available. This gives us:
- ‚úÖ Industry-standard observability
- ‚úÖ Distributed tracing support
- ‚úÖ Span creation for requests
- ‚úÖ Trace context propagation
- ‚úÖ Integration with existing telemetry

### Competitive Context

Having OpenTelemetry built-in gives us a strong advantage:
- Matches enterprise observability standards
- Enables integration with existing monitoring stacks (Datadog, New Relic, Honeycomb)
- Differentiates from simpler libraries (LiteLLM.js, llm.js)
- Complements our existing telemetry middleware

### What Exists
- ‚úÖ OpenTelemetry middleware (`packages/middleware/src/opentelemetry.ts`)
- ‚úÖ Telemetry middleware
- ‚úÖ Metrics and events system
- ‚úÖ Provenance tracking
- ‚úÖ Documentation (`docs/opentelemetry.md`)

### Enhancement Opportunities

1. **Install Dependencies**
   ```bash
   npm install @opentelemetry/api
   npm install @opentelemetry/sdk-node
   npm install @opentelemetry/instrumentation
   npm install @opentelemetry/exporter-trace-otlp-http
   ```

2. **OpenTelemetry Middleware**
   ```typescript
   // src/middleware/opentelemetry.ts
   export interface OpenTelemetryConfig {
     serviceName: string;
     endpoint: string; // OTLP endpoint
     headers?: Record<string, string>;
     samplingRate?: number;
   }

   export function createOpenTelemetryMiddleware(config)
   ```

3. **Span Creation**
   - Create span for each request
   - Child spans for middleware
   - Child spans for adapter calls
   - Add attributes (provider, model, tokens)

4. **Trace Context Propagation**
   - Extract trace context from headers
   - Inject trace context into outgoing requests
   - Support W3C Trace Context standard

5. **Metrics Export**
   - Request count
   - Latency histogram
   - Error rate
   - Token usage

6. **Integration Examples**
   - Jaeger example
   - Zipkin example
   - Datadog example
   - Honeycomb example
   - New Relic example

7. **Documentation**
   - `docs/opentelemetry.md`
   - Setup guide
   - Configuration reference
   - Integration examples

### Deliverables
- OpenTelemetry middleware
- Span creation and propagation
- Metrics export
- Integration examples
- Documentation

### Success Criteria
- ‚úÖ OpenTelemetry integration working end-to-end
- ‚úÖ OpenTelemetry span creation < 1ms
- ‚úÖ OpenTelemetry setup guide

### Risk Assessment
| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| OpenTelemetry version conflicts | Medium | Medium | Pin versions, test compatibility |
| Performance overhead from instrumentation | Medium | Medium | Make profiling opt-in, optimize critical paths |


## Interactive Examples & Tutorials

**Status:** ‚úÖ **FEASIBLE**
**Competitive Priority:** HIGH - Documentation quality is critical

### Competitive Context

LangChain.js has excellent documentation and is well-established in the ecosystem. Vercel AI SDK has comprehensive examples and video content. Our documentation quality needs to match these leaders. Interactive examples would:
- Lower the learning curve (currently a weakness)
- Compete with Vercel's playground experience
- Build community (like "Awesome LangChain" lists)
- Improve SEO and discovery

### Existing Foundation
- ‚úÖ EXAMPLES.md with 16+ examples
- ‚úÖ Working code examples
- ‚úÖ Documentation system
- ‚úÖ Tutorial video synopses (42 videos planned)

### What's Needed
- Interactive code playground (web-based)
- Step-by-step tutorials
- Video script creation
- Screen recording and editing
- Hosting platform (YouTube, docs site)

### Benefits
- Lower learning curve
- Better onboarding
- Community growth
- SEO and discovery

### Implementation Tasks

1. **Interactive Playground (Web)**
   - Create Next.js app
   - Monaco editor for code
   - Live execution
   - Share examples via URL
   - Deploy to Vercel

2. **Step-by-Step Tutorials**
   - Tutorial 1: Getting Started
   - Tutorial 2: Routing & Fallbacks
   - Tutorial 3: Middleware
   - Tutorial 4: Streaming
   - Tutorial 5: Cost Optimization
   - Tutorial 6: Custom Adapters

3. **Video Tutorial Scripts**
   - Write detailed scripts (see `docs/TUTORIAL-VIDEO-SYNOPSES.md` - 42 videos planned)
   - Include code examples
   - Talking points
   - Visual aids needed

4. **Video Production** (External)
   - Screen recordings
   - Voiceover
   - Editing
   - Publishing to YouTube

5. **Update Documentation Site**
   - Add tutorials section
   - Embed videos
   - Interactive code snippets
   - Search functionality

6. **Create "Awesome ai.matey" List**
   - Community projects
   - Blog posts
   - Tutorials
   - Adapters
   - Tools

### Deliverables
- Interactive playground
- 6 step-by-step tutorials
- Video scripts (42 videos, see [TUTORIAL-VIDEO-SYNOPSES.md](./TUTORIAL-VIDEO-SYNOPSES.md))
- Documentation updates
- Community resource list

### Success Criteria
- ‚úÖ Interactive playground deployed
- ‚úÖ 20+ video tutorials published
- ‚úÖ Playground loads in < 2 seconds
- ‚úÖ Video tutorials for all major features

### Risk Assessment
| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| Video production takes significant effort | High | Low | Focus on most important features first, external help |
| Playground requires maintenance | Medium | Low | Keep simple, use established frameworks |
