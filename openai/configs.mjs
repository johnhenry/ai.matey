export const ollama = {
    endpoint: "http://localhost:11434",
    model: "llama3.2:latest",
};
export const mistral = {
    endpoint: "https://api.mistral.ai",
    model: "mistral-small-latest",
};
export const groq = {
    endpoint: "https://api.groq.com/openai",
    model: "llama3-8b-8192",
};
export const nvidia = {
    endpoint: "https://integrate.api.nvidia.com",
    model: "meta/llama-3.1-8b-instruct",
};
export default {
    endpoint: "https://api.openai.com",
    model: "gpt-4o-mini",
}