"use strict";(globalThis.webpackChunkai_matey_docs=globalThis.webpackChunkai_matey_docs||[]).push([[3549],{1018:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"tutorials/beginner/building-chat-api","title":"Tutorial 04: Building a Chat API","description":"Learn how to build a production-ready HTTP API server for chat completions using Express and ai.matey.","source":"@site/docs/tutorials/beginner/04-building-chat-api.md","sourceDirName":"tutorials/beginner","slug":"/tutorials/beginner/building-chat-api","permalink":"/ai.matey/tutorials/beginner/building-chat-api","draft":false,"unlisted":false,"editUrl":"https://github.com/johnhenry/ai.matey/tree/main/packages/ai.matey.docs/docs/tutorials/beginner/04-building-chat-api.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"docs","previous":{"title":"Tutorial 03: Multi-Provider Routing","permalink":"/ai.matey/tutorials/beginner/multi-provider"},"next":{"title":"Intermediate Representation (IR) Format","permalink":"/ai.matey/guides/architecture/ir-format"}}');var s=r(1085),a=r(1184);const i={sidebar_position:4},o="Tutorial 04: Building a Chat API",d={},l=[{value:"What You&#39;ll Build",id:"what-youll-build",level:2},{value:"Time Required",id:"time-required",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"What We&#39;re Building",id:"what-were-building",level:2},{value:"Step 1: Install Dependencies",id:"step-1-install-dependencies",level:2},{value:"Step 2: Create Basic Server",id:"step-2-create-basic-server",level:2},{value:"Step 3: Test It",id:"step-3-test-it",level:2},{value:"Step 4: Add Streaming Support",id:"step-4-add-streaming-support",level:2},{value:"Step 5: Add Request Validation",id:"step-5-add-request-validation",level:2},{value:"Step 6: Add Rate Limiting",id:"step-6-add-rate-limiting",level:2},{value:"Step 7: Add Authentication (Optional)",id:"step-7-add-authentication-optional",level:2},{value:"Step 8: Add Middleware",id:"step-8-add-middleware",level:2},{value:"Step 9: Add Multi-Provider Support",id:"step-9-add-multi-provider-support",level:2},{value:"Complete Production Server",id:"complete-production-server",level:2},{value:"Testing Your API",id:"testing-your-api",level:2},{value:"cURL",id:"curl",level:3},{value:"JavaScript Client",id:"javascript-client",level:3},{value:"Streaming Client",id:"streaming-client",level:3},{value:"Next Steps",id:"next-steps",level:2},{value:"Deployment",id:"deployment",level:2},{value:"Docker",id:"docker",level:3},{value:"Vercel",id:"vercel",level:3}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"tutorial-04-building-a-chat-api",children:"Tutorial 04: Building a Chat API"})}),"\n",(0,s.jsx)(n.p,{children:"Learn how to build a production-ready HTTP API server for chat completions using Express and ai.matey."}),"\n",(0,s.jsx)(n.h2,{id:"what-youll-build",children:"What You'll Build"}),"\n",(0,s.jsx)(n.p,{children:"A complete chat API with:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"REST endpoint"})," for chat completions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Streaming support"})," with Server-Sent Events"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Error handling"})," and validation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Rate limiting"})," and security"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"OpenAI-compatible"})," API format"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"time-required",children:"Time Required"}),"\n",(0,s.jsxs)(n.p,{children:["\u23f1\ufe0f ",(0,s.jsx)(n.strong,{children:"25 minutes"})]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Completed ",(0,s.jsx)(n.a,{href:"/tutorials/beginner/multi-provider",children:"Tutorial 03: Multi-Provider Routing"})]}),"\n",(0,s.jsx)(n.li,{children:"Basic understanding of HTTP and REST APIs"}),"\n",(0,s.jsx)(n.li,{children:"Node.js 18+ installed"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"what-were-building",children:"What We're Building"}),"\n",(0,s.jsxs)(n.p,{children:["An HTTP server that exposes an OpenAI-compatible ",(0,s.jsx)(n.code,{children:"/v1/chat/completions"})," endpoint:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'curl http://localhost:3000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "gpt-4",\n    "messages": [{"role": "user", "content": "Hello!"}]\n  }\'\n'})}),"\n",(0,s.jsx)(n.h2,{id:"step-1-install-dependencies",children:"Step 1: Install Dependencies"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"mkdir ai-matey-api\ncd ai-matey-api\nnpm init -y\nnpm install express cors dotenv\nnpm install ai.matey.core ai.matey.frontend ai.matey.backend ai.matey.middleware\nnpm install -D @types/express @types/cors\n"})}),"\n",(0,s.jsx)(n.h2,{id:"step-2-create-basic-server",children:"Step 2: Create Basic Server"}),"\n",(0,s.jsxs)(n.p,{children:["Create ",(0,s.jsx)(n.code,{children:"server.js"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"import 'dotenv/config';\nimport express from 'express';\nimport cors from 'cors';\nimport { Bridge } from 'ai.matey.core';\nimport { OpenAIFrontendAdapter } from 'ai.matey.frontend/openai';\nimport { AnthropicBackendAdapter } from 'ai.matey.backend/anthropic';\n\nconst app = express();\nconst PORT = process.env.PORT || 3000;\n\n// Middleware\napp.use(cors());\napp.use(express.json());\n\n// Create bridge\nconst bridge = new Bridge(\n  new OpenAIFrontendAdapter(),\n  new AnthropicBackendAdapter({\n    apiKey: process.env.ANTHROPIC_API_KEY\n  })\n);\n\n// Health check\napp.get('/health', (req, res) => {\n  res.json({ status: 'ok', timestamp: Date.now() });\n});\n\n// Chat completions endpoint\napp.post('/v1/chat/completions', async (req, res) => {\n  try {\n    const response = await bridge.chat(req.body);\n    res.json(response);\n  } catch (error) {\n    console.error('Error:', error);\n    res.status(500).json({\n      error: {\n        message: error.message,\n        type: 'api_error'\n      }\n    });\n  }\n});\n\n// Start server\napp.listen(PORT, () => {\n  console.log(`\ud83d\ude80 Server running on http://localhost:${PORT}`);\n  console.log(`\ud83d\udcdd API: http://localhost:${PORT}/v1/chat/completions`);\n});\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Create ",(0,s.jsx)(n.code,{children:".env"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ANTHROPIC_API_KEY=your_api_key_here\nPORT=3000\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Update ",(0,s.jsx)(n.code,{children:"package.json"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n  "name": "ai-matey-api",\n  "version": "1.0.0",\n  "type": "module",\n  "scripts": {\n    "start": "node server.js",\n    "dev": "node --watch server.js"\n  },\n  "dependencies": {\n    "express": "^4.18.0",\n    "cors": "^2.8.5",\n    "dotenv": "^16.0.0",\n    "ai.matey.core": "latest",\n    "ai.matey.frontend": "latest",\n    "ai.matey.backend": "latest"\n  }\n}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"step-3-test-it",children:"Step 3: Test It"}),"\n",(0,s.jsx)(n.p,{children:"Start the server:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"npm start\n"})}),"\n",(0,s.jsx)(n.p,{children:"Test with curl:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:3000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "gpt-4",\n    "messages": [\n      {"role": "user", "content": "What is ai.matey?"}\n    ]\n  }\'\n'})}),"\n",(0,s.jsx)(n.h2,{id:"step-4-add-streaming-support",children:"Step 4: Add Streaming Support"}),"\n",(0,s.jsx)(n.p,{children:"Streaming allows clients to receive responses in real-time:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"import 'dotenv/config';\nimport express from 'express';\nimport cors from 'cors';\nimport { Bridge } from 'ai.matey.core';\nimport { OpenAIFrontendAdapter } from 'ai.matey.frontend/openai';\nimport { AnthropicBackendAdapter } from 'ai.matey.backend/anthropic';\n\nconst app = express();\napp.use(cors());\napp.use(express.json());\n\nconst bridge = new Bridge(\n  new OpenAIFrontendAdapter(),\n  new AnthropicBackendAdapter({\n    apiKey: process.env.ANTHROPIC_API_KEY\n  })\n);\n\napp.post('/v1/chat/completions', async (req, res) => {\n  try {\n    // Check if streaming is requested\n    if (req.body.stream) {\n      // Set headers for Server-Sent Events\n      res.setHeader('Content-Type', 'text/event-stream');\n      res.setHeader('Cache-Control', 'no-cache');\n      res.setHeader('Connection', 'keep-alive');\n\n      const stream = await bridge.chatStream(req.body);\n\n      for await (const chunk of stream) {\n        // Send chunk as SSE\n        res.write(`data: ${JSON.stringify(chunk)}\\n\\n`);\n      }\n\n      // Send done signal\n      res.write('data: [DONE]\\n\\n');\n      res.end();\n    } else {\n      // Non-streaming response\n      const response = await bridge.chat(req.body);\n      res.json(response);\n    }\n  } catch (error) {\n    console.error('Error:', error);\n\n    if (!res.headersSent) {\n      res.status(500).json({\n        error: {\n          message: error.message,\n          type: 'api_error'\n        }\n      });\n    } else {\n      // If streaming already started, send error event\n      res.write(`data: ${JSON.stringify({ error: error.message })}\\n\\n`);\n      res.end();\n    }\n  }\n});\n\napp.listen(3000, () => {\n  console.log('\ud83d\ude80 Server running on http://localhost:3000');\n});\n"})}),"\n",(0,s.jsx)(n.p,{children:"Test streaming:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:3000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "gpt-4",\n    "messages": [{"role": "user", "content": "Count to 5"}],\n    "stream": true\n  }\'\n'})}),"\n",(0,s.jsx)(n.h2,{id:"step-5-add-request-validation",children:"Step 5: Add Request Validation"}),"\n",(0,s.jsx)(n.p,{children:"Validate incoming requests:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// Validation middleware\nfunction validateChatRequest(req, res, next) {\n  const { model, messages } = req.body;\n\n  // Check required fields\n  if (!model) {\n    return res.status(400).json({\n      error: {\n        message: 'Missing required field: model',\n        type: 'invalid_request_error'\n      }\n    });\n  }\n\n  if (!messages || !Array.isArray(messages)) {\n    return res.status(400).json({\n      error: {\n        message: 'Missing or invalid field: messages',\n        type: 'invalid_request_error'\n      }\n    });\n  }\n\n  if (messages.length === 0) {\n    return res.status(400).json({\n      error: {\n        message: 'messages array cannot be empty',\n        type: 'invalid_request_error'\n      }\n    });\n  }\n\n  // Validate message format\n  for (const msg of messages) {\n    if (!msg.role || !msg.content) {\n      return res.status(400).json({\n        error: {\n          message: 'Each message must have role and content',\n          type: 'invalid_request_error'\n        }\n      });\n    }\n\n    if (!['system', 'user', 'assistant'].includes(msg.role)) {\n      return res.status(400).json({\n        error: {\n          message: `Invalid role: ${msg.role}`,\n          type: 'invalid_request_error'\n        }\n      });\n    }\n  }\n\n  next();\n}\n\n// Use validation\napp.post('/v1/chat/completions', validateChatRequest, async (req, res) => {\n  // ... rest of handler\n});\n"})}),"\n",(0,s.jsx)(n.h2,{id:"step-6-add-rate-limiting",children:"Step 6: Add Rate Limiting"}),"\n",(0,s.jsx)(n.p,{children:"Protect your API from abuse:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"npm install express-rate-limit\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"import rateLimit from 'express-rate-limit';\n\n// Rate limiter\nconst limiter = rateLimit({\n  windowMs: 15 * 60 * 1000, // 15 minutes\n  max: 100, // Max 100 requests per window\n  message: {\n    error: {\n      message: 'Too many requests, please try again later',\n      type: 'rate_limit_exceeded'\n    }\n  },\n  standardHeaders: true,\n  legacyHeaders: false,\n});\n\n// Apply to all routes\napp.use('/v1/', limiter);\n"})}),"\n",(0,s.jsx)(n.h2,{id:"step-7-add-authentication-optional",children:"Step 7: Add Authentication (Optional)"}),"\n",(0,s.jsx)(n.p,{children:"Require API keys:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// API key middleware\nfunction requireApiKey(req, res, next) {\n  const apiKey = req.headers['authorization']?.replace('Bearer ', '');\n\n  if (!apiKey) {\n    return res.status(401).json({\n      error: {\n        message: 'Missing API key',\n        type: 'authentication_error'\n      }\n    });\n  }\n\n  // Validate API key (in production, check against database)\n  const validKeys = process.env.VALID_API_KEYS?.split(',') || [];\n\n  if (!validKeys.includes(apiKey)) {\n    return res.status(401).json({\n      error: {\n        message: 'Invalid API key',\n        type: 'authentication_error'\n      }\n    });\n  }\n\n  next();\n}\n\n// Require auth for chat endpoint\napp.post('/v1/chat/completions', requireApiKey, validateChatRequest, async (req, res) => {\n  // ... rest of handler\n});\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Update ",(0,s.jsx)(n.code,{children:".env"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"VALID_API_KEYS=sk-test-123,sk-test-456\n"})}),"\n",(0,s.jsx)(n.p,{children:"Test with API key:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:3000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -H "Authorization: Bearer sk-test-123" \\\n  -d \'{\n    "model": "gpt-4",\n    "messages": [{"role": "user", "content": "Hello"}]\n  }\'\n'})}),"\n",(0,s.jsx)(n.h2,{id:"step-8-add-middleware",children:"Step 8: Add Middleware"}),"\n",(0,s.jsx)(n.p,{children:"Use ai.matey middleware for production features:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"import {\n  createLoggingMiddleware,\n  createCachingMiddleware,\n  createCostTrackingMiddleware\n} from 'ai.matey.middleware';\n\n// Add middleware to bridge\nbridge.use(createLoggingMiddleware({\n  level: 'info',\n  redactFields: ['apiKey', 'authorization']\n}));\n\nbridge.use(createCachingMiddleware({\n  ttl: 3600,\n  maxSize: 1000\n}));\n\nbridge.use(createCostTrackingMiddleware({\n  budgetLimit: 100,\n  onBudgetExceeded: () => {\n    console.error('\u26a0\ufe0f  Daily budget exceeded!');\n  }\n}));\n"})}),"\n",(0,s.jsx)(n.h2,{id:"step-9-add-multi-provider-support",children:"Step 9: Add Multi-Provider Support"}),"\n",(0,s.jsx)(n.p,{children:"Use Router for high availability:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"import { Router } from 'ai.matey.core';\nimport { OpenAIFrontendAdapter } from 'ai.matey.frontend/openai';\nimport { AnthropicBackendAdapter } from 'ai.matey.backend/anthropic';\nimport { OpenAIBackendAdapter } from 'ai.matey.backend/openai';\n\nconst router = new Router(new OpenAIFrontendAdapter(), {\n  backends: [\n    new AnthropicBackendAdapter({\n      apiKey: process.env.ANTHROPIC_API_KEY\n    }),\n    new OpenAIBackendAdapter({\n      apiKey: process.env.OPENAI_API_KEY\n    })\n  ],\n  strategy: 'priority',\n  fallbackOnError: true\n});\n\n// Use router instead of bridge\napp.post('/v1/chat/completions', async (req, res) => {\n  const response = req.body.stream\n    ? await router.chatStream(req.body)\n    : await router.chat(req.body);\n  // ... handle response\n});\n"})}),"\n",(0,s.jsx)(n.h2,{id:"complete-production-server",children:"Complete Production Server"}),"\n",(0,s.jsx)(n.p,{children:"Here's the full production-ready server:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// server.js\nimport 'dotenv/config';\nimport express from 'express';\nimport cors from 'cors';\nimport rateLimit from 'express-rate-limit';\nimport { Router } from 'ai.matey.core';\nimport { OpenAIFrontendAdapter } from 'ai.matey.frontend/openai';\nimport { AnthropicBackendAdapter } from 'ai.matey.backend/anthropic';\nimport { OpenAIBackendAdapter } from 'ai.matey.backend/openai';\nimport {\n  createLoggingMiddleware,\n  createCachingMiddleware,\n  createRetryMiddleware\n} from 'ai.matey.middleware';\n\nconst app = express();\nconst PORT = process.env.PORT || 3000;\n\n// Express middleware\napp.use(cors());\napp.use(express.json({ limit: '1mb' }));\n\n// Rate limiting\nconst limiter = rateLimit({\n  windowMs: 15 * 60 * 1000,\n  max: 100,\n  message: { error: { message: 'Too many requests', type: 'rate_limit_exceeded' } }\n});\napp.use('/v1/', limiter);\n\n// Create router\nconst router = new Router(new OpenAIFrontendAdapter(), {\n  backends: [\n    new AnthropicBackendAdapter({ apiKey: process.env.ANTHROPIC_API_KEY }),\n    new OpenAIBackendAdapter({ apiKey: process.env.OPENAI_API_KEY })\n  ],\n  strategy: 'priority',\n  fallbackOnError: true,\n  healthCheck: { enabled: true, interval: 60000 }\n});\n\n// Add ai.matey middleware\nrouter.use(createLoggingMiddleware({ level: 'info', redactFields: ['apiKey'] }));\nrouter.use(createRetryMiddleware({ maxAttempts: 3 }));\nrouter.use(createCachingMiddleware({ ttl: 3600 }));\n\n// Validation middleware\nfunction validateChatRequest(req, res, next) {\n  const { model, messages } = req.body;\n\n  if (!model || !messages || !Array.isArray(messages) || messages.length === 0) {\n    return res.status(400).json({\n      error: { message: 'Invalid request', type: 'invalid_request_error' }\n    });\n  }\n\n  next();\n}\n\n// Routes\napp.get('/health', (req, res) => {\n  res.json({ status: 'ok', timestamp: Date.now() });\n});\n\napp.post('/v1/chat/completions', validateChatRequest, async (req, res) => {\n  try {\n    if (req.body.stream) {\n      res.setHeader('Content-Type', 'text/event-stream');\n      res.setHeader('Cache-Control', 'no-cache');\n      res.setHeader('Connection', 'keep-alive');\n\n      const stream = await router.chatStream(req.body);\n\n      for await (const chunk of stream) {\n        res.write(`data: ${JSON.stringify(chunk)}\\n\\n`);\n      }\n\n      res.write('data: [DONE]\\n\\n');\n      res.end();\n    } else {\n      const response = await router.chat(req.body);\n      res.json(response);\n    }\n  } catch (error) {\n    console.error('Error:', error);\n\n    if (!res.headersSent) {\n      res.status(500).json({\n        error: { message: error.message, type: 'api_error' }\n      });\n    }\n  }\n});\n\n// Error handler\napp.use((err, req, res, next) => {\n  console.error('Unhandled error:', err);\n  res.status(500).json({\n    error: { message: 'Internal server error', type: 'server_error' }\n  });\n});\n\n// Start server\napp.listen(PORT, () => {\n  console.log(`\ud83d\ude80 Server running on http://localhost:${PORT}`);\n  console.log(`\ud83d\udcdd Chat API: POST http://localhost:${PORT}/v1/chat/completions`);\n  console.log(`\u2764\ufe0f  Health: GET http://localhost:${PORT}/health`);\n});\n\n// Graceful shutdown\nprocess.on('SIGTERM', () => {\n  console.log('SIGTERM received, shutting down gracefully');\n  process.exit(0);\n});\n"})}),"\n",(0,s.jsx)(n.h2,{id:"testing-your-api",children:"Testing Your API"}),"\n",(0,s.jsx)(n.h3,{id:"curl",children:"cURL"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Non-streaming\ncurl -X POST http://localhost:3000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{"model":"gpt-4","messages":[{"role":"user","content":"Hello"}]}\'\n\n# Streaming\ncurl -X POST http://localhost:3000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{"model":"gpt-4","messages":[{"role":"user","content":"Count to 5"}],"stream":true}\'\n'})}),"\n",(0,s.jsx)(n.h3,{id:"javascript-client",children:"JavaScript Client"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// client.js\nasync function chat(message) {\n  const response = await fetch('http://localhost:3000/v1/chat/completions', {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({\n      model: 'gpt-4',\n      messages: [{ role: 'user', content: message }]\n    })\n  });\n\n  const data = await response.json();\n  return data.choices[0].message.content;\n}\n\nconsole.log(await chat('What is ai.matey?'));\n"})}),"\n",(0,s.jsx)(n.h3,{id:"streaming-client",children:"Streaming Client"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"async function chatStream(message) {\n  const response = await fetch('http://localhost:3000/v1/chat/completions', {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({\n      model: 'gpt-4',\n      messages: [{ role: 'user', content: message }],\n      stream: true\n    })\n  });\n\n  const reader = response.body.getReader();\n  const decoder = new TextDecoder();\n\n  while (true) {\n    const { done, value } = await reader.read();\n    if (done) break;\n\n    const chunk = decoder.decode(value);\n    const lines = chunk.split('\\n');\n\n    for (const line of lines) {\n      if (line.startsWith('data: ')) {\n        const data = line.slice(6);\n        if (data === '[DONE]') return;\n\n        const json = JSON.parse(data);\n        const content = json.choices?.[0]?.delta?.content;\n        if (content) {\n          process.stdout.write(content);\n        }\n      }\n    }\n  }\n}\n\nawait chatStream('Count to 10');\n"})}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(n.p,{children:"Congratulations! You've built a production-ready chat API."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Explore more:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/johnhenry/ai.matey/tree/main/packages/ai.matey.docs/examples/05-http-servers",children:"HTTP Server Examples"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"/patterns",children:"Integration Patterns"})}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"deployment",children:"Deployment"}),"\n",(0,s.jsx)(n.h3,{id:"docker",children:"Docker"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-dockerfile",children:'FROM node:18-alpine\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production\nCOPY . .\nEXPOSE 3000\nCMD ["node", "server.js"]\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"docker build -t ai-matey-api .\ndocker run -p 3000:3000 -e ANTHROPIC_API_KEY=your_key ai-matey-api\n"})}),"\n",(0,s.jsx)(n.h3,{id:"vercel",children:"Vercel"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n  "version": 2,\n  "builds": [{ "src": "server.js", "use": "@vercel/node" }],\n  "routes": [{ "src": "/(.*)", "dest": "/server.js" }]\n}\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\ud83c\udf89 Tutorial Complete!"})," You've mastered the ai.matey basics. Explore ",(0,s.jsx)(n.a,{href:"https://github.com/johnhenry/ai.matey/tree/main/packages/ai.matey.docs/examples/07-advanced-patterns",children:"Advanced Examples"})]})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},1184:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>o});var t=r(4041);const s={},a=t.createContext(s);function i(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);