"use strict";(globalThis.webpackChunkai_matey_docs=globalThis.webpackChunkai_matey_docs||[]).push([[347],{1184:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>d});var t=r(4041);const i={},a=t.createContext(i);function s(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(a.Provider,{value:n},e.children)}},1415:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>d,default:()=>p,frontMatter:()=>s,metadata:()=>t,toc:()=>o});const t=JSON.parse('{"id":"packages/frontend","title":"ai.matey.frontend","description":"Frontend adapters define the input format for your AI requests. Write code in any API format you prefer - OpenAI, Anthropic, Google Gemini, or others.","source":"@site/docs/packages/frontend.md","sourceDirName":"packages","slug":"/packages/frontend","permalink":"/ai.matey/packages/frontend","draft":false,"unlisted":false,"editUrl":"https://github.com/johnhenry/ai.matey/tree/main/packages/ai.matey.docs/docs/packages/frontend.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"docs","previous":{"title":"ai.matey.core","permalink":"/ai.matey/packages/core"},"next":{"title":"ai.matey.backend","permalink":"/ai.matey/packages/backend"}}');var i=r(1085),a=r(1184);const s={sidebar_position:3},d="ai.matey.frontend",l={},o=[{value:"Installation",id:"installation",level:2},{value:"Overview",id:"overview",level:2},{value:"OpenAI Frontend Adapter",id:"openai-frontend-adapter",level:2},{value:"Installation",id:"installation-1",level:3},{value:"Usage",id:"usage",level:3},{value:"Supported Features",id:"supported-features",level:3},{value:"Request Format",id:"request-format",level:3},{value:"Anthropic Frontend Adapter",id:"anthropic-frontend-adapter",level:2},{value:"Installation",id:"installation-2",level:3},{value:"Usage",id:"usage-1",level:3},{value:"Key Differences from OpenAI",id:"key-differences-from-openai",level:3},{value:"Supported Features",id:"supported-features-1",level:3},{value:"Google Gemini Frontend Adapter",id:"google-gemini-frontend-adapter",level:2},{value:"Installation",id:"installation-3",level:3},{value:"Usage",id:"usage-2",level:3},{value:"Key Differences",id:"key-differences",level:3},{value:"Supported Features",id:"supported-features-2",level:3},{value:"Ollama Frontend Adapter",id:"ollama-frontend-adapter",level:2},{value:"Installation",id:"installation-4",level:3},{value:"Usage",id:"usage-3",level:3},{value:"Supported Features",id:"supported-features-3",level:3},{value:"Cohere Frontend Adapter",id:"cohere-frontend-adapter",level:2},{value:"Installation",id:"installation-5",level:3},{value:"Usage",id:"usage-4",level:3},{value:"Key Differences",id:"key-differences-1",level:3},{value:"Mistral Frontend Adapter",id:"mistral-frontend-adapter",level:2},{value:"Installation",id:"installation-6",level:3},{value:"Usage",id:"usage-5",level:3},{value:"Groq Frontend Adapter",id:"groq-frontend-adapter",level:2},{value:"Installation",id:"installation-7",level:3},{value:"Usage",id:"usage-6",level:3},{value:"Choosing a Frontend Adapter",id:"choosing-a-frontend-adapter",level:2},{value:"Use OpenAI Frontend Adapter if:",id:"use-openai-frontend-adapter-if",level:3},{value:"Use Anthropic Frontend Adapter if:",id:"use-anthropic-frontend-adapter-if",level:3},{value:"Use Gemini Frontend Adapter if:",id:"use-gemini-frontend-adapter-if",level:3},{value:"Use Ollama Frontend Adapter if:",id:"use-ollama-frontend-adapter-if",level:3},{value:"Feature Compatibility Matrix",id:"feature-compatibility-matrix",level:2},{value:"Creating Custom Frontend Adapters",id:"creating-custom-frontend-adapters",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Choose Based on Your Codebase",id:"1-choose-based-on-your-codebase",level:3},{value:"2. Frontend is Independent of Backend",id:"2-frontend-is-independent-of-backend",level:3},{value:"3. Type Safety",id:"3-type-safety",level:3},{value:"Semantic Drift",id:"semantic-drift",level:2},{value:"Example: System Messages",id:"example-system-messages",level:3},{value:"Handling Drift",id:"handling-drift",level:3},{value:"See Also",id:"see-also",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"aimateyfrontend",children:"ai.matey.frontend"})}),"\n",(0,i.jsx)(n.p,{children:"Frontend adapters define the input format for your AI requests. Write code in any API format you prefer - OpenAI, Anthropic, Google Gemini, or others."}),"\n",(0,i.jsx)(n.h2,{id:"installation",children:"Installation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"npm install ai.matey.frontend\n"})}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"Frontend adapters translate your chosen API format into ai.matey's Intermediate Representation (IR). This allows you to write code in whatever format you're most comfortable with."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Available Adapters:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"OpenAI"}),"\n",(0,i.jsx)(n.li,{children:"Anthropic"}),"\n",(0,i.jsx)(n.li,{children:"Google Gemini"}),"\n",(0,i.jsx)(n.li,{children:"Ollama"}),"\n",(0,i.jsx)(n.li,{children:"Cohere"}),"\n",(0,i.jsx)(n.li,{children:"Mistral"}),"\n",(0,i.jsx)(n.li,{children:"Groq"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"openai-frontend-adapter",children:"OpenAI Frontend Adapter"}),"\n",(0,i.jsx)(n.p,{children:"Use OpenAI's API format as input."}),"\n",(0,i.jsx)(n.h3,{id:"installation-1",children:"Installation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"import { OpenAIFrontendAdapter } from 'ai.matey.frontend/openai';\n"})}),"\n",(0,i.jsx)(n.h3,{id:"usage",children:"Usage"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"import { Bridge } from 'ai.matey.core';\nimport { OpenAIFrontendAdapter } from 'ai.matey.frontend/openai';\nimport { AnthropicBackendAdapter } from 'ai.matey.backend/anthropic';\n\nconst bridge = new Bridge(\n  new OpenAIFrontendAdapter(),\n  new AnthropicBackendAdapter({ apiKey: 'your-key' })\n);\n\n// Write in OpenAI format\nconst response = await bridge.chat({\n  model: 'gpt-4',\n  messages: [\n    { role: 'system', content: 'You are helpful.' },\n    { role: 'user', content: 'Hello!' }\n  ],\n  temperature: 0.7,\n  max_tokens: 100\n});\n"})}),"\n",(0,i.jsx)(n.h3,{id:"supported-features",children:"Supported Features"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"\u2705 Chat completions"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Streaming"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Function calling/tools"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Vision (image inputs)"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 System messages"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Temperature, top_p, max_tokens"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Stop sequences"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Presence/frequency penalties"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"request-format",children:"Request Format"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"interface OpenAIChatRequest {\n  model: string;\n  messages: OpenAIMessage[];\n  temperature?: number;\n  max_tokens?: number;\n  top_p?: number;\n  n?: number;\n  stream?: boolean;\n  stop?: string | string[];\n  presence_penalty?: number;\n  frequency_penalty?: number;\n  logit_bias?: Record<string, number>;\n  user?: string;\n  tools?: OpenAITool[];\n  tool_choice?: 'none' | 'auto' | { type: 'function'; function: { name: string } };\n}\n"})}),"\n",(0,i.jsx)(n.h2,{id:"anthropic-frontend-adapter",children:"Anthropic Frontend Adapter"}),"\n",(0,i.jsx)(n.p,{children:"Use Anthropic's API format as input."}),"\n",(0,i.jsx)(n.h3,{id:"installation-2",children:"Installation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"import { AnthropicFrontendAdapter } from 'ai.matey.frontend/anthropic';\n"})}),"\n",(0,i.jsx)(n.h3,{id:"usage-1",children:"Usage"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"import { Bridge } from 'ai.matey.core';\nimport { AnthropicFrontendAdapter } from 'ai.matey.frontend/anthropic';\nimport { OpenAIBackendAdapter } from 'ai.matey.backend/openai';\n\nconst bridge = new Bridge(\n  new AnthropicFrontendAdapter(),\n  new OpenAIBackendAdapter({ apiKey: 'your-key' })\n);\n\n// Write in Anthropic format\nconst response = await bridge.chat({\n  model: 'claude-3-5-sonnet-20241022',\n  max_tokens: 100,\n  messages: [\n    { role: 'user', content: 'Hello!' }\n  ],\n  system: 'You are helpful.', // System message separate\n  temperature: 0.7\n});\n"})}),"\n",(0,i.jsx)(n.h3,{id:"key-differences-from-openai",children:"Key Differences from OpenAI"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["System messages are a separate ",(0,i.jsx)(n.code,{children:"system"})," parameter (not in messages array)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"max_tokens"})," is ",(0,i.jsx)(n.strong,{children:"required"})]}),"\n",(0,i.jsxs)(n.li,{children:["No ",(0,i.jsx)(n.code,{children:"presence_penalty"})," or ",(0,i.jsx)(n.code,{children:"frequency_penalty"})]}),"\n",(0,i.jsx)(n.li,{children:"Different tool/function calling format"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"supported-features-1",children:"Supported Features"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"\u2705 Chat completions"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Streaming"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Tool use"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Vision (image inputs)"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 System messages (as parameter)"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Temperature, top_p, top_k"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Stop sequences"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"google-gemini-frontend-adapter",children:"Google Gemini Frontend Adapter"}),"\n",(0,i.jsx)(n.p,{children:"Use Google's Gemini API format as input."}),"\n",(0,i.jsx)(n.h3,{id:"installation-3",children:"Installation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"import { GeminiFrontendAdapter } from 'ai.matey.frontend/gemini';\n"})}),"\n",(0,i.jsx)(n.h3,{id:"usage-2",children:"Usage"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"import { Bridge } from 'ai.matey.core';\nimport { GeminiFrontendAdapter } from 'ai.matey.frontend/gemini';\nimport { OpenAIBackendAdapter } from 'ai.matey.backend/openai';\n\nconst bridge = new Bridge(\n  new GeminiFrontendAdapter(),\n  new OpenAIBackendAdapter({ apiKey: 'your-key' })\n);\n\n// Write in Gemini format\nconst response = await bridge.chat({\n  model: 'gemini-1.5-pro',\n  contents: [\n    {\n      role: 'user',\n      parts: [{ text: 'Hello!' }]\n    }\n  ],\n  generationConfig: {\n    temperature: 0.7,\n    maxOutputTokens: 100\n  }\n});\n"})}),"\n",(0,i.jsx)(n.h3,{id:"key-differences",children:"Key Differences"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Uses ",(0,i.jsx)(n.code,{children:"contents"})," instead of ",(0,i.jsx)(n.code,{children:"messages"})]}),"\n",(0,i.jsxs)(n.li,{children:["Uses ",(0,i.jsx)(n.code,{children:"parts"})," for multi-modal content"]}),"\n",(0,i.jsxs)(n.li,{children:["Role is ",(0,i.jsx)(n.code,{children:"model"})," instead of ",(0,i.jsx)(n.code,{children:"assistant"})]}),"\n",(0,i.jsxs)(n.li,{children:["Configuration in ",(0,i.jsx)(n.code,{children:"generationConfig"})," object"]}),"\n",(0,i.jsx)(n.li,{children:"System instructions separate parameter"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"supported-features-2",children:"Supported Features"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"\u2705 Chat completions"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Streaming"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Function calling"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Vision (native multi-modal support)"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 System instructions"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Temperature, topP, topK"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Stop sequences"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Safety settings"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"ollama-frontend-adapter",children:"Ollama Frontend Adapter"}),"\n",(0,i.jsx)(n.p,{children:"Use Ollama's API format (compatible with local models)."}),"\n",(0,i.jsx)(n.h3,{id:"installation-4",children:"Installation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"import { OllamaFrontendAdapter } from 'ai.matey.frontend/ollama';\n"})}),"\n",(0,i.jsx)(n.h3,{id:"usage-3",children:"Usage"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"import { Bridge } from 'ai.matey.core';\nimport { OllamaFrontendAdapter } from 'ai.matey.frontend/ollama';\nimport { OpenAIBackendAdapter } from 'ai.matey.backend/openai';\n\nconst bridge = new Bridge(\n  new OllamaFrontendAdapter(),\n  new OpenAIBackendAdapter({ apiKey: 'your-key' })\n);\n\n// Write in Ollama format\nconst response = await bridge.chat({\n  model: 'llama3.2',\n  messages: [\n    { role: 'user', content: 'Hello!' }\n  ],\n  stream: false\n});\n"})}),"\n",(0,i.jsx)(n.h3,{id:"supported-features-3",children:"Supported Features"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"\u2705 Chat completions"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Streaming"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 System messages"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Temperature, top_p, top_k"}),"\n",(0,i.jsx)(n.li,{children:"\u26a0\ufe0f Limited tool support (model-dependent)"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"cohere-frontend-adapter",children:"Cohere Frontend Adapter"}),"\n",(0,i.jsx)(n.p,{children:"Use Cohere's API format as input."}),"\n",(0,i.jsx)(n.h3,{id:"installation-5",children:"Installation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"import { CohereFrontendAdapter } from 'ai.matey.frontend/cohere';\n"})}),"\n",(0,i.jsx)(n.h3,{id:"usage-4",children:"Usage"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"import { Bridge } from 'ai.matey.core';\nimport { CohereFrontendAdapter } from 'ai.matey.frontend/cohere';\nimport { OpenAIBackendAdapter } from 'ai.matey.backend/openai';\n\nconst bridge = new Bridge(\n  new CohereFrontendAdapter(),\n  new OpenAIBackendAdapter({ apiKey: 'your-key' })\n);\n\n// Write in Cohere format\nconst response = await bridge.chat({\n  model: 'command-r-plus',\n  message: 'Hello!',\n  chat_history: [\n    { role: 'USER', message: 'Hi' },\n    { role: 'CHATBOT', message: 'Hello!' }\n  ],\n  temperature: 0.7\n});\n"})}),"\n",(0,i.jsx)(n.h3,{id:"key-differences-1",children:"Key Differences"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Uses ",(0,i.jsx)(n.code,{children:"message"})," for current user message"]}),"\n",(0,i.jsx)(n.li,{children:"Chat history separate from current message"}),"\n",(0,i.jsxs)(n.li,{children:["Roles are ",(0,i.jsx)(n.code,{children:"USER"})," and ",(0,i.jsx)(n.code,{children:"CHATBOT"})," (uppercase)"]}),"\n",(0,i.jsx)(n.li,{children:"Different parameter names"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"mistral-frontend-adapter",children:"Mistral Frontend Adapter"}),"\n",(0,i.jsx)(n.p,{children:"Use Mistral's API format (very similar to OpenAI)."}),"\n",(0,i.jsx)(n.h3,{id:"installation-6",children:"Installation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"import { MistralFrontendAdapter } from 'ai.matey.frontend/mistral';\n"})}),"\n",(0,i.jsx)(n.h3,{id:"usage-5",children:"Usage"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"import { Bridge } from 'ai.matey.core';\nimport { MistralFrontendAdapter } from 'ai.matey.frontend/mistral';\nimport { OpenAIBackendAdapter } from 'ai.matey.backend/openai';\n\nconst bridge = new Bridge(\n  new MistralFrontendAdapter(),\n  new OpenAIBackendAdapter({ apiKey: 'your-key' })\n);\n\n// Mistral format is very similar to OpenAI\nconst response = await bridge.chat({\n  model: 'mistral-large-latest',\n  messages: [\n    { role: 'user', content: 'Hello!' }\n  ],\n  temperature: 0.7\n});\n"})}),"\n",(0,i.jsx)(n.h2,{id:"groq-frontend-adapter",children:"Groq Frontend Adapter"}),"\n",(0,i.jsx)(n.p,{children:"Use Groq's API format (OpenAI-compatible)."}),"\n",(0,i.jsx)(n.h3,{id:"installation-7",children:"Installation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"import { GroqFrontendAdapter } from 'ai.matey.frontend/groq';\n"})}),"\n",(0,i.jsx)(n.h3,{id:"usage-6",children:"Usage"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"import { Bridge } from 'ai.matey.core';\nimport { GroqFrontendAdapter } from 'ai.matey.frontend/groq';\nimport { AnthropicBackendAdapter } from 'ai.matey.backend/anthropic';\n\nconst bridge = new Bridge(\n  new GroqFrontendAdapter(),\n  new AnthropicBackendAdapter({ apiKey: 'your-key' })\n);\n\n// Groq uses OpenAI-compatible format\nconst response = await bridge.chat({\n  model: 'llama3-8b-8192',\n  messages: [\n    { role: 'user', content: 'Hello!' }\n  ]\n});\n"})}),"\n",(0,i.jsx)(n.h2,{id:"choosing-a-frontend-adapter",children:"Choosing a Frontend Adapter"}),"\n",(0,i.jsx)(n.h3,{id:"use-openai-frontend-adapter-if",children:"Use OpenAI Frontend Adapter if:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"You're familiar with OpenAI's API"}),"\n",(0,i.jsx)(n.li,{children:"You want the most widely-used format"}),"\n",(0,i.jsx)(n.li,{children:"Your codebase already uses OpenAI"}),"\n",(0,i.jsx)(n.li,{children:"You need maximum compatibility"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"use-anthropic-frontend-adapter-if",children:"Use Anthropic Frontend Adapter if:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"You prefer Anthropic's API design"}),"\n",(0,i.jsx)(n.li,{children:"You want explicit system message separation"}),"\n",(0,i.jsx)(n.li,{children:"Your codebase uses Claude"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"use-gemini-frontend-adapter-if",children:"Use Gemini Frontend Adapter if:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"You're working with Google AI Platform"}),"\n",(0,i.jsx)(n.li,{children:"You need native multi-modal support"}),"\n",(0,i.jsx)(n.li,{children:"You use Vertex AI"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"use-ollama-frontend-adapter-if",children:"Use Ollama Frontend Adapter if:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"You're working with local models"}),"\n",(0,i.jsx)(n.li,{children:"You want a simple format"}),"\n",(0,i.jsx)(n.li,{children:"You're developing locally"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"feature-compatibility-matrix",children:"Feature Compatibility Matrix"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Feature"}),(0,i.jsx)(n.th,{children:"OpenAI"}),(0,i.jsx)(n.th,{children:"Anthropic"}),(0,i.jsx)(n.th,{children:"Gemini"}),(0,i.jsx)(n.th,{children:"Ollama"}),(0,i.jsx)(n.th,{children:"Cohere"}),(0,i.jsx)(n.th,{children:"Mistral"}),(0,i.jsx)(n.th,{children:"Groq"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Chat"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"\u2705"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Streaming"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"\u2705"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Tools/Functions"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"\u26a0\ufe0f"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"\u26a0\ufe0f"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Vision"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"\u26a0\ufe0f"}),(0,i.jsx)(n.td,{children:"\u274c"}),(0,i.jsx)(n.td,{children:"\u26a0\ufe0f"}),(0,i.jsx)(n.td,{children:"\u26a0\ufe0f"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"System Messages"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"\u26a0\ufe0f"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"\u2705"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:"\u2705 Fully supported | \u26a0\ufe0f Partially supported | \u274c Not supported"}),"\n",(0,i.jsx)(n.h2,{id:"creating-custom-frontend-adapters",children:"Creating Custom Frontend Adapters"}),"\n",(0,i.jsx)(n.p,{children:"You can create your own frontend adapter:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"import { FrontendAdapter } from 'ai.matey.core';\nimport type { IRChatCompletionRequest, IRChatCompletionResponse } from 'ai.matey.types';\n\nexport class CustomFrontendAdapter implements FrontendAdapter {\n  name = 'custom';\n\n  // Convert custom format to IR\n  toIR(request: CustomRequest): IRChatCompletionRequest {\n    return {\n      model: request.modelName,\n      messages: request.conversation.map(msg => ({\n        role: msg.sender === 'human' ? 'user' : 'assistant',\n        content: msg.text\n      })),\n      temperature: request.temp,\n      max_tokens: request.maxLength\n    };\n  }\n\n  // Convert IR back to custom format\n  fromIR(response: IRChatCompletionResponse): CustomResponse {\n    return {\n      reply: response.choices[0].message.content,\n      tokens: response.usage?.total_tokens || 0\n    };\n  }\n\n  // Stream support (optional)\n  async *fromIRStream(stream: AsyncIterable<IRChatCompletionChunk>) {\n    for await (const chunk of stream) {\n      yield {\n        text: chunk.choices?.[0]?.delta?.content || '',\n        done: chunk.choices?.[0]?.finish_reason !== null\n      };\n    }\n  }\n}\n"})}),"\n",(0,i.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsx)(n.h3,{id:"1-choose-based-on-your-codebase",children:"1. Choose Based on Your Codebase"}),"\n",(0,i.jsx)(n.p,{children:"If your codebase already uses a specific API format, use that frontend adapter:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"// Existing OpenAI code\nconst openaiResponse = await openai.chat.completions.create({\n  model: 'gpt-4',\n  messages: [{ role: 'user', content: 'Hello' }]\n});\n\n// Easy migration - use OpenAI frontend adapter\nconst bridge = new Bridge(\n  new OpenAIFrontendAdapter(), // Same format!\n  new AnthropicBackendAdapter({ apiKey })\n);\n"})}),"\n",(0,i.jsx)(n.h3,{id:"2-frontend-is-independent-of-backend",children:"2. Frontend is Independent of Backend"}),"\n",(0,i.jsx)(n.p,{children:"Mix and match any frontend with any backend:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"// OpenAI format \u2192 Anthropic backend\nnew Bridge(new OpenAIFrontendAdapter(), new AnthropicBackendAdapter({ apiKey }));\n\n// Anthropic format \u2192 OpenAI backend\nnew Bridge(new AnthropicFrontendAdapter(), new OpenAIBackendAdapter({ apiKey }));\n\n// Gemini format \u2192 Groq backend\nnew Bridge(new GeminiFrontendAdapter(), new GroqBackendAdapter({ apiKey }));\n"})}),"\n",(0,i.jsx)(n.h3,{id:"3-type-safety",children:"3. Type Safety"}),"\n",(0,i.jsx)(n.p,{children:"Use TypeScript for frontend-specific request types:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"import type { OpenAIChatRequest, OpenAIChatResponse } from 'ai.matey.frontend/openai';\n\nconst bridge = new Bridge(\n  new OpenAIFrontendAdapter(),\n  new AnthropicBackendAdapter({ apiKey })\n);\n\nconst request: OpenAIChatRequest = {\n  model: 'gpt-4',\n  messages: [{ role: 'user', content: 'Hello' }]\n};\n\nconst response: OpenAIChatResponse = await bridge.chat(request);\n"})}),"\n",(0,i.jsx)(n.h2,{id:"semantic-drift",children:"Semantic Drift"}),"\n",(0,i.jsx)(n.p,{children:'When converting between formats, some features may not map perfectly. This is called "semantic drift."'}),"\n",(0,i.jsx)(n.h3,{id:"example-system-messages",children:"Example: System Messages"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"// OpenAI: system messages in array\n{\n  messages: [\n    { role: 'system', content: 'You are helpful.' },\n    { role: 'user', content: 'Hello' }\n  ]\n}\n\n// Anthropic: system separate\n{\n  system: 'You are helpful.',\n  messages: [\n    { role: 'user', content: 'Hello' }\n  ]\n}\n"})}),"\n",(0,i.jsx)(n.p,{children:"The adapter handles this automatically, but be aware of potential drift."}),"\n",(0,i.jsx)(n.h3,{id:"handling-drift",children:"Handling Drift"}),"\n",(0,i.jsx)(n.p,{children:"Frontend adapters track and warn about semantic drift:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-typescript",children:"const response = await bridge.chat(request);\n\nif (response.warnings) {\n  console.warn('Semantic drift detected:', response.warnings);\n}\n"})}),"\n",(0,i.jsx)(n.h2,{id:"see-also",children:"See Also"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"/packages/backend",children:"Backend Adapters"})," - Available backend providers"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"/guides/architecture/ir-format",children:"IR Format"})," - Intermediate representation details"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"/packages/core",children:"Core Package"})," - Bridge and Router documentation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"/tutorials/beginner/simple-bridge",children:"Tutorial: Simple Bridge"})," - Step-by-step guide"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}}}]);